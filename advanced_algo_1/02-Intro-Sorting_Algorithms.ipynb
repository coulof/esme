{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 02 Advanced Algorithm - Introduction - Sorting algorithms\n",
    "***\n",
    "\n",
    "This introduction supplement the official ESME Course and takes inspiration from [CS50](https://cs50.harvard.edu/x/2024/weeks/3/) & [Open data structures](https://opendatastructures.org/)\n",
    "\n",
    "\n",
    "## The Need for Efficiency\n",
    "\n",
    "Why do we need efficiency when we have so powerful computers ?\n",
    "\n",
    "![Moore's law illustration](Moores_Law_Transistor_Count_1970-2020.png)\n",
    "\n",
    "\n",
    "### Number of operations\n",
    "* one million items dataset\n",
    "* Let's say we search one by one it takes up to one million operation\n",
    "** This is a linear search that takes $O(n)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000000e+12\n"
     ]
    }
   ],
   "source": [
    "dataset = 1_000_000\n",
    "search_operations = 1_000_000\n",
    "total_operations = dataset*search_operations\n",
    "print(f\"{total_operations:e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processor speeds\n",
    "* 1 Gigahertz =~ 1 billion operations per seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.7 minutes\n"
     ]
    }
   ],
   "source": [
    "print(f\"{total_operations/1_000_000_000/60:.3} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Who in the room has the patience to wait 10 or even 5 minutes to get a result from its app ?\n",
    "\n",
    "### Planet scale app like Google\n",
    "Google indexed 8.5 Billions pages and receives more than 64000 queries per second.\n",
    "\n",
    "If it were to keep up with linear search (which is quite good!) it would need hundreds of thousands of servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big O notation\n",
    "\n",
    "### Definition\n",
    "Big O notation is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity.\n",
    "\n",
    "### For us in Computer Science\n",
    "It describes the performance or complexity of an algorithm.\n",
    "\n",
    "It can be use to measure time of execution and memory space.\n",
    "\n",
    "It doesn't have to be precise\n",
    "\n",
    "### Important values\n",
    "\n",
    "![](big_O.png)\n",
    "\n",
    "Big-O | Name | Description\n",
    "------| ---- | -----------\n",
    "**O(1)** | constant | **This is the best.** The algorithm always takes the same amount of time, regardless of how much data there is. Example: looking up an element of an array by its index.\n",
    "**O(log n)** | logarithmic | **Pretty great.** These kinds of algorithms halve the amount of data with each iteration. If you have 100 items, it takes about 7 steps to find the answer. With 1,000 items, it takes 10 steps. And 1,000,000 items only take 20 steps. This is super fast even for large amounts of data. Example: binary search.\n",
    "**O(n)** | linear | **Good performance.** If you have 100 items, this does 100 units of work. Doubling the number of items makes the algorithm take exactly twice as long (200 units of work). Example: sequential search.\n",
    "**O(n log n)** | \"linearithmic\" | **Decent performance.** This is slightly worse than linear but not too bad. Example: the fastest general-purpose sorting algorithms.\n",
    "**O(n^2)** | quadratic | **Kinda slow.** If you have 100 items, this does 100^2 = 10,000 units of work. Doubling the number of items makes it four times slower (because 2 squared equals 4). Example: algorithms using nested loops, such as insertion sort.\n",
    "**O(n^3)** | cubic | **Poor performance.** If you have 100 items, this does 100^3 = 1,000,000 units of work. Doubling the input size makes it eight times slower. Example: matrix multiplication.\n",
    "**O(2^n)** | exponential | **Very poor performance.** You want to avoid these kinds of algorithms, but sometimes you have no choice. Adding just one bit to the input doubles the running time. Example: traveling salesperson problem.\n",
    "**O(n!)** | factorial | **Intolerably slow.** It literally takes a million years to do anything.\n",
    "\n",
    "Copy-Pasted from [https://github.com/kodecocodes/swift-algorithm-club](https://github.com/kodecocodes/swift-algorithm-club)\n",
    "<br>\n",
    "> Can somebody remind me a search algorithm we implemented during the reharsal session on reccursion ? What is its complexity ?\n",
    "\n",
    "## Conclusion\n",
    "The goal of the next course is to [study different algorithms](https://www.cs.usfca.edu/~galles/visualization/ComparisonSort.html) and evaluate their complexity."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
